#!/usr/bin/env python
# encoding: utf-8

"""
@Author: yangwenhao
@Contact: 874681044@qq.com
@Software: PyCharm
@File: KaldiDataset.py
@Time: 2019/12/10 下午9:28
@Overview:
"""
import json
import os
import pathlib
import pdb
import random
import pandas as pd
# import kaldi_io
import kaldiio
import numpy as np
import torch
import torch.utils.data as data
# from kaldi_io import read_mat
from tqdm import tqdm
import h5py

import Process_Data.constants as c


def check_exist(path):
    if not os.path.exists(path):
        raise FileExistsError(path)


def write_vec_ark(uid, feats, write_path, set_id):
    """
    :param uid: utterance ids generated by dataset class
    :param feat: features
    :param write_path:
    :param id: train or test
    :return:
    """
    file_path = pathlib.Path(write_path)
    if not file_path.exists():
        os.makedirs(str(file_path))

    assert len(uid) == len(feats)
    print('There are %d vectors.' % len(uid))
    ark_file = write_path + '/xvec.{}.ark'.format(set_id)
    scp_file = write_path + '/xvec.{}.scp'.format(set_id)

    # write scp and ark file
    with kaldiio.WriteHelper('ark,scp:%s,%s'%(ark_file, scp_file)) as writer:
    # with open(scp_file, 'w') as scp, open(ark_file, 'wb') as ark:
        for i in range(len(uid)):
            vec = feats[i]
            # len_vec = len(vec.tobytes())
            # key = uid[i]
            # kaldiio..write_vec_flt(ark, vec, key=key)
            # # print(ark.tell())
            # scp.write(str(uid[i]) + ' ' + str(ark_file) +
            #           ':' + str(ark.tell()-len_vec-10) + '\n')
            writer(uid[i], vec)

    print('\nark,scp files are in: {}, {}.'.format(ark_file, scp_file))

    # Prepare utt2spk file
    # if set=='train':
    #     utt2spk_file = write_path+'/utt2spk'
    #     with open(utt2spk_file, 'w') as utt2spk:
    #         for i in range(len(uid)):
    #             spk = uid[i].split('-')[0]
    #             utt2spk.write(str(uid[i]) + ' ' + str(spk)+'\n')
    #
    #     print('utt2spk file is in: {}.'.format(utt2spk_file))


class KaldiTrainDataset(data.Dataset):
    def __init__(self, dir, samples_per_speaker, transform, num_valid=5):

        feat_scp = dir + '/feats.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(spk2utt):
            raise FileExistsError(spk2utt)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split(' ')
                spk_name = spk_utt[0]
                if spk_name not in dataset.keys():
                    spk_utt[-1] = spk_utt[-1].rstrip('\n')
                    dataset[spk_name] = spk_utt[1:]

        utt2spk_dict = {}
        with open(utt2spk, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                utt_spk = line.split(' ')
                uid = utt_spk[0]
                if uid not in utt2spk_dict.keys():
                    utt_spk[-1] = utt_spk[-1].rstrip('\n')
                    utt2spk_dict[uid] = utt_spk[-1]
        # pdb.set_trace()

        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        print('==> There are {} speakers in Dataset.'.format(len(speakers)))
        spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
        idx_to_spk = {i: speakers[i] for i in range(len(speakers))}

        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        pbar = tqdm(enumerate(kaldiio.load_scp(feat_scp)), ncols=100)

        for idx, (utt_id, feat) in pbar:
            uid2feat[utt_id] = feat

        print('\tThere are {} utterances in Train Dataset.'.format(len(uid2feat)))
        valid_set = {}
        valid_uid2feat = {}
        valid_utt2spk_dict = {}

        for spk in speakers:
            if spk not in valid_set.keys():
                valid_set[spk] = []
                for i in range(num_valid):
                    if len(dataset[spk]) <= 1:
                        break
                    j = np.random.randint(len(dataset[spk]))
                    utt = dataset[spk].pop(j)
                    valid_set[spk].append(utt)

                    valid_uid2feat[valid_set[spk][-1]
                                   ] = uid2feat.pop(valid_set[spk][-1])
                    valid_utt2spk_dict[utt] = utt2spk_dict[utt]

        print('\tSpliting {} utterances for Validation.\n'.format(
            len(valid_uid2feat)))

        self.feat_dim = uid2feat[dataset[speakers[0]][0]].shape[1]
        self.speakers = speakers
        self.dataset = dataset
        self.valid_set = valid_set
        self.valid_uid2feat = valid_uid2feat
        self.valid_utt2spk_dict = valid_utt2spk_dict
        self.uid2feat = uid2feat
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_spks = len(speakers)
        self.transform = transform
        self.samples_per_speaker = samples_per_speaker

    def __getitem__(self, sid):
        sid %= self.num_spks
        spk = self.idx_to_spk[sid]
        utts = self.dataset[spk]
        n_samples = 0
        y = np.array([[]]).reshape(0, self.feat_dim)

        frames = c.N_SAMPLES
        while n_samples < frames:

            uid = random.randrange(0, len(utts))
            feature = self.uid2feat[utts[uid]]

            # Get the index of feature
            if n_samples == 0:
                start = int(random.uniform(0, len(feature)))
            else:
                start = 0
            stop = int(
                min(len(feature) - 1, max(1.0, start + frames - n_samples)))
            try:
                y = np.concatenate((y, feature[start:stop]), axis=0)
            except:
                pdb.set_trace()
            n_samples = len(y)
            # transform features if required

        feature = self.transform(y)
        label = sid
        return feature, label

    def __len__(self):
        return self.samples_per_speaker * len(self.speakers)  # 返回一个epoch的采样数


class KaldiValidDataset(data.Dataset):
    def __init__(self, valid_set, spk_to_idx, valid_uid2feat, valid_utt2spk_dict, transform):

        speakers = [spk for spk in valid_set.keys()]
        speakers.sort()
        self.speakers = speakers
        self.dataset = valid_set
        self.valid_set = valid_set
        self.uid2feat = valid_uid2feat
        self.utt2spk_dict = valid_utt2spk_dict
        self.spk_to_idx = spk_to_idx
        self.num_spks = len(speakers)
        self.transform = transform

    def __getitem__(self, index):
        uid = list(self.uid2feat.keys())[index]
        spk = self.utt2spk_dict[uid]

        feature = self.transform(self.uid2feat[uid])
        label = self.spk_to_idx[spk]

        return feature, label

    def __len__(self):
        return len(self.uid2feat)


class KaldiTestDataset(data.Dataset):
    def __init__(self, dir, transform):

        feat_scp = dir + '/feats.scp'
        spk2utt = dir + '/spk2utt'
        trials = dir + '/trials'

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(spk2utt):
            raise FileExistsError(spk2utt)
        if not os.path.exists(trials):
            raise FileExistsError(trials)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split(' ')
                spk_name = spk_utt[0]
                if spk_name not in dataset.keys():
                    spk_utt[-1] = spk_utt[-1].rstrip('\n')
                    dataset[spk_name] = spk_utt[1:]

        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        print('==> There are {} speakers in Test Dataset.'.format(len(speakers)))

        uid2feat = {}
        for utt_id, feat in kaldiio.load_scp(feat_scp):
            uid2feat[utt_id] = feat
        print('\tThere are {} utterances in Test Dataset.'.format(len(uid2feat)))

        trials_pair = []
        with open(trials, 'r') as t:
            all_pairs = t.readlines()
            for line in all_pairs:
                pair = line.split(' ')
                if pair[2] == 'nontarget\n':
                    pair_true = False
                else:
                    pair_true = True

                trials_pair.append((pair[0], pair[1], pair_true))

        print('\tThere are {} pairs in test Dataset.\n'.format(len(trials_pair)))

        self.feat_dim = uid2feat[dataset[speakers[0]][0]].shape[1]
        self.speakers = speakers
        self.uid2feat = uid2feat
        self.trials_pair = trials_pair
        self.num_spks = len(speakers)
        self.transform = transform

    def __getitem__(self, index):
        uid_a, uid_b, label = self.trials_pair[index]

        data_a = self.uid2feat[uid_a]
        data_b = self.uid2feat[uid_b]

        data_a = self.transform(data_a)
        data_b = self.transform(data_b)

        return data_a, data_b, label

    def __len__(self):
        return len(self.trials_pair)


class TrainDataset(data.Dataset):
    def __init__(self, dir, transform):

        feat_scp = dir + '/feats.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'
        num_valid = 5

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(spk2utt):
            raise FileExistsError(spk2utt)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split(' ')
                spk_name = spk_utt[0]
                if spk_name not in dataset.keys():
                    spk_utt[-1] = spk_utt[-1].rstrip('\n')
                    dataset[spk_name] = spk_utt[1:]
        utt2spk_dict = {}
        with open(utt2spk, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                utt_spk = line.split(' ')
                uid = utt_spk[0]
                if uid not in utt2spk_dict.keys():
                    utt_spk[-1] = utt_spk[-1].rstrip('\n')
                    utt2spk_dict[uid] = utt_spk[-1]
        # pdb.set_trace()

        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        print('==>There are {} speakers in Dataset.'.format(len(speakers)))
        spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
        idx_to_spk = {i: speakers[i] for i in range(len(speakers))}

        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        pbar = tqdm(enumerate(kaldiio.load_scp(feat_scp)), ncols=100)
        for idx, (utt_id, feat) in pbar:
            uid2feat[utt_id] = feat

        print('==>There are {} utterances in Train Dataset.'.format(len(uid2feat)))
        valid_set = {}
        valid_uid2feat = {}
        valid_utt2spk_dict = {}

        for spk in speakers:
            if spk not in valid_set.keys():
                valid_set[spk] = []
                for i in range(num_valid):
                    if len(dataset[spk]) <= 1:
                        break
                    j = np.random.randint(len(dataset[spk]))
                    utt = dataset[spk].pop(j)
                    valid_set[spk].append(utt)

                    valid_uid2feat[valid_set[spk][-1]
                                   ] = uid2feat.pop(valid_set[spk][-1])
                    valid_utt2spk_dict[utt] = utt2spk_dict[utt]

        print('==>Spliting {} utterances for Validation.\n'.format(
            len(valid_uid2feat)))
        utt_lst = []
        for uid in list(utt2spk_dict.keys()):
            if uid not in valid_uid2feat.keys():
                utt_lst.append(uid)
        random.shuffle(utt_lst)

        self.feat_dim = uid2feat[dataset[speakers[0]][0]].shape[1]
        self.speakers = speakers
        self.dataset = dataset
        self.utt_lst = utt_lst
        self.utt2spk_dict = utt2spk_dict
        self.valid_set = valid_set
        self.valid_uid2feat = valid_uid2feat
        self.valid_utt2spk_dict = valid_utt2spk_dict
        self.uid2feat = uid2feat
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_spks = len(speakers)
        self.transform = transform

    def __getitem__(self, sid):
        uid = self.utt_lst[sid]
        spk = self.utt2spk_dict[uid]

        feat = self.uid2feat[uid]
        feature = self.transform(feat)

        label = self.spk_to_idx[spk]
        return feature, label

    def __len__(self):
        return len(self.utt_lst)  # 返回一个epoch的采样数


class KaldiTupleDataset(data.Dataset):
    def __init__(self, dir, transform, samples_per_spk=150, num_valid=5, num_enroll=5, nagative_pair=1):

        feat_scp = dir + '/feats.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'
        train_trials = dir + '/train.trials'

        self.num_enroll = num_enroll

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(spk2utt):
            raise FileExistsError(spk2utt)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split(' ')
                spk_name = spk_utt[0]
                if spk_name not in dataset.keys():
                    spk_utt[-1] = spk_utt[-1].rstrip('\n')
                    dataset[spk_name] = spk_utt[1:]

        utt2spk_dict = {}
        with open(utt2spk, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                utt_spk = line.split(' ')
                uid = utt_spk[0]
                if uid not in utt2spk_dict.keys():
                    utt_spk[-1] = utt_spk[-1].rstrip('\n')
                    utt2spk_dict[uid] = utt_spk[-1]
        # pdb.set_trace()

        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        print('==> There are {} speakers in Dataset.'.format(len(speakers)))
        spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
        idx_to_spk = {i: speakers[i] for i in range(len(speakers))}

        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        pbar = tqdm(enumerate(kaldiio.load_scp(feat_scp)), ncols=100)
        for idx, (utt_id, feat) in pbar:
            uid2feat[utt_id] = feat

        print('\tThere are {} utterances in Train Dataset.'.format(len(uid2feat)))
        valid_set = {}
        valid_uid2feat = {}
        valid_utt2spk_dict = {}

        for spk in speakers:
            if spk not in valid_set.keys():
                valid_set[spk] = []
                for i in range(num_valid):
                    if len(dataset[spk]) <= 1:
                        break
                    j = np.random.randint(len(dataset[spk]))
                    utt = dataset[spk].pop(j)
                    valid_set[spk].append(utt)

                    valid_uid2feat[valid_set[spk][-1]
                                   ] = uid2feat.pop(valid_set[spk][-1])
                    valid_utt2spk_dict[utt] = utt2spk_dict[utt]

        print('\tSpliting {} utterances for Validation.\n'.format(
            len(valid_uid2feat)))

        tuple_lst = []
        train_trials_f = open(train_trials, 'w')
        for i in range(len(speakers)):
            spk = speakers[i]
            for j in range(samples_per_spk):
                eval_utts = dataset[spk].copy()
                positive_eval_idx = np.random.randint(0, len(eval_utts))
                eval_utt = eval_utts[positive_eval_idx]
                eval_utts.pop(positive_eval_idx)
                positive_enroll = np.random.choice(eval_utts, size=num_enroll)

                positive_trials = []
                positive_trials.append(eval_utt)
                for x in positive_enroll:
                    positive_trials.append(x)

                positive_trials.append('1')
                positive_trials.append(str(spk_to_idx[spk]))
                for m in range(num_enroll):
                    positive_trials.append(str(spk_to_idx[spk]))

                tuple_lst.append(positive_trials)
                train_trials_f.write(' '.join(positive_trials) + '\n')

                nagative_spks = speakers.copy()
                nagative_spks.pop(i)

                nagative_spks = np.random.choice(
                    nagative_spks, size=nagative_pair, replace=False)
                for nagative_spk in nagative_spks:
                    negative_trials = []
                    negative_trials.append(eval_utt)
                    nagative_enroll = np.random.choice(
                        dataset[nagative_spk], size=num_enroll)
                    for x in nagative_enroll:
                        negative_trials.append(x)

                    negative_trials.append('0')
                    negative_trials.append(str(spk_to_idx[spk]))
                    for m in range(num_enroll):
                        negative_trials.append(str(spk_to_idx[nagative_spk]))

                    tuple_lst.append(negative_trials)
                    train_trials_f.write(' '.join(negative_trials) + '\n')

        train_trials_f.close()

        print('\tGenerate {} tuples for training.\n'.format(len(tuple_lst)))

        self.feat_dim = uid2feat[dataset[speakers[0]][0]].shape[1]
        self.speakers = speakers
        self.dataset = dataset
        self.valid_set = valid_set
        self.valid_uid2feat = valid_uid2feat
        self.valid_utt2spk_dict = valid_utt2spk_dict
        self.uid2feat = uid2feat
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_spks = len(speakers)
        self.transform = transform
        self.samples_per_spk = samples_per_spk
        self.tuple_lst = tuple_lst

    def __getitem__(self, sid):
        # pdb.set_trace()
        pairs = self.tuple_lst[sid]
        uids = pairs[:self.num_enroll+1]
        labels = pairs[self.num_enroll+1:]
        labels = [int(x) for x in labels]

        feat_uids = [self.uid2feat[uid] for uid in uids]
        feats = [self.transform(feat) for feat in feat_uids]
        features = torch.cat(feats, dim=0)
        # features = np.concatenate(feat_uids, axis=0)
        labels = torch.LongTensor(labels)

        # features:   eval_utt,    utt1,  ...  ,    utt5
        # labels:0/1, spk_idx1,spk_idx2,  ...  ,spk_idx2
        return features, labels

    def __len__(self):
        return len(self.tuple_lst)  # 返回一个epoch的采样数


class KaldiExtractDataset(data.Dataset):
    def __init__(self, dir, transform, filer_loader, trials_file='trials',
                 extract_trials=True, vad_select=False, feat_type='kaldi',
                 verbose=0):

        feat_scp = dir + '/feats.scp' if feat_type != 'wav' else dir + '/wav.scp'
        trials = dir + '/%s' % trials_file
        vad_scp = dir + '/vad.scp'

        if os.path.isfile(trials) and extract_trials:
            assert os.path.exists(feat_scp), feat_scp

            trials_utts = set()
            with open(trials, 'r') as u:
                all_cls = u.readlines()
                for line in all_cls:
                    try:
                        utt_a, utt_b, target = line.split()
                    except Exception as e:
                        print("error: [", line, "]")
                        raise e

                    trials_utts.add(utt_a)
                    trials_utts.add(utt_b)

            uid2feat = {}
            with open(feat_scp, 'r') as u:
                all_cls = tqdm(
                    u.readlines(), ncols=100) if verbose > 0 else u.readlines()
                for line in all_cls:
                    # utt_path = line.split(' ')
                    utt_path = line.split()
                    upath_idx = 11 if len(utt_path) > 2 else 1

                    uid = utt_path[0]
                    if uid in trials_utts:
                        uid2feat[uid] = utt_path[-upath_idx]

        else:
            if verbose > 0:
                print("    trials not exist, extract xvector for all utterances!")
            uid2feat = {}
            with open(feat_scp, 'r') as u:
                all_cls = tqdm(
                    u.readlines(), ncols=100) if verbose > 0 else u.readlines()
                for line in all_cls:
                    # utt_path = line.split(' ')
                    utt_path = line.split()
                    uid = utt_path[0]
                    uid2feat[uid] = utt_path[-1]

        uid2vad = {}
        if vad_select:
            if verbose > 0:
                print("    Select voiced frames to extracting xvectors!")
            assert os.path.exists(vad_scp), vad_scp
            # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
            with open(vad_scp, 'r') as f:
                for line in f.readlines():
                    uid_vad = line.split()
                    uid, vad_offset = uid_vad
                    uid2vad[uid] = vad_offset

        # pdb.set_trace()
        utts = list(uid2feat.keys())
        utts.sort()
        # assert len(utts) == len(trials_utts)
        if verbose > 0:
            print('==> There are {} utterances in Verifcation set to extract vectors.'.format(
                len(utts)))

        self.uid2feat = uid2feat
        self.uid2vad = uid2vad
        self.transform = transform
        self.uids = utts
        self.file_loader = filer_loader

    def __getitem__(self, index):
        uid = self.uids[index]
        y = self.file_loader(self.uid2feat[uid])

        if uid in self.uid2vad:
            voice_idx = np.where(kaldiio.load_mat(self.uid2vad[uid]) == 1)[0]
            y = y[voice_idx]

        feature = self.transform(y)

        return feature, uid

    def __len__(self):
        return len(self.uids)  # 返回一个epoch的采样数


class ScriptVerifyDataset(data.Dataset):
    def __init__(self, dir, xvectors_dir, trials_file='trials', loader=np.load, return_uid=False):

        feat_scp = xvectors_dir + '/xvectors.scp'
        trials = dir + '/%s' % trials_file

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(trials):
            raise FileExistsError(trials)

        uid2feat = {}
        with open(feat_scp, 'r') as f:
            for line in f.readlines():
                uid, feat_offset = line.split()
                uid2feat[uid] = feat_offset

        utts = set(uid2feat.keys())

        # print('\n==> There are {} utterances in Verification trials.'.format(len(uid2feat)))

        trials_pair = []
        positive_pairs = 0
        skip_pairs = 0
        assert os.path.exists(trials)
        with open(trials, 'r') as t:
            all_pairs = t.readlines()
            for line in all_pairs:
                pair = line.split()
                if pair[2] in ['nontarget', '0']:
                    pair_true = False
                else:
                    pair_true = True
                    positive_pairs += 1

                if pair[0] in utts and pair[1] in utts:
                    trials_pair.append((pair[0], pair[1], pair_true))
                else:
                    skip_pairs += 1

        trials_pair = np.array(trials_pair)
        assert len(trials_pair) > 0
        # trials_pair = trials_pair[trials_pair[:, 2].argsort()[::-1]]
        # print('    There are {} pairs in trials with {} positive pairs'.format(len(trials_pair),
        #                                                                        positive_pairs))

        self.uid2feat = uid2feat
        self.trials_pair = trials_pair
        self.numofpositive = positive_pairs

        self.loader = loader
        self.return_uid = return_uid

    def __getitem__(self, index):
        uid_a, uid_b, label = self.trials_pair[index]

        feat_a = self.uid2feat[uid_a]
        feat_b = self.uid2feat[uid_b]
        data_a = self.loader(feat_a)
        data_b = self.loader(feat_b)

        if label in ['True', True]:
            label = True
        else:
            label = False

        if self.return_uid:
            # pdb.set_trace()
            # print(uid_a, uid_b)
            return data_a, data_b, label, uid_a, uid_b

        return data_a, data_b, label

    def partition(self, num):
        if num > len(self.trials_pair):
            print('%d is greater than the total number of pairs')

        else:
            self.trials_pair = self.trials_pair[:num]

        assert len(self.trials_pair) == num
        num_positive = 0
        for x, y, z in self.trials_pair:
            if z in ['True', True]:
                num_positive += 1

        assert len(self.trials_pair) == num, '%d != %d' % (
            len(self.trials_pair), num)
        assert self.numofpositive == num_positive, '%d != %d' % (
            self.numofpositive, num_positive)
        print('%d positive pairs remain.' % num_positive)

    def __len__(self):
        return len(self.trials_pair)


class ScriptTrainDataset(data.Dataset):
    def __init__(self, dir, samples_per_speaker, transform, num_valid=5, feat_type='kaldi',
                 loader=np.load, return_uid=False, domain=False, rand_test=False,
                 vad_select=False, sample_type='instance', sr=16000, save_dir='',
                 segment_len=c.N_SAMPLES, segment_shift=c.N_SAMPLES, verbose=1, min_frames=0):
        self.return_uid = return_uid
        self.domain = domain
        self.rand_test = rand_test
        self.segment_len = segment_len
        self.segment_shift = segment_shift
        self.min_frames = min_frames

        self.feat_type = feat_type
        self.sample_type = sample_type  # balance or instance

        feat_scp = dir + '/feats.scp' if feat_type != 'wav' else dir + '/wav.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'
        utt2num_frames = dir + '/utt2num_frames' if feat_type != 'wav' else dir + '/utt2dur'
        utt2dom = dir + '/utt2dom'
        vad_scp = dir + '/vad.scp'

        assert os.path.exists(feat_scp), feat_scp
        assert os.path.exists(spk2utt), spk2utt

        invalid_uid = set([])
        base_utts = []

        uid2vad = {}
        if vad_select:
            assert os.path.exists(vad_scp), vad_scp
            # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
            with open(vad_scp, 'r') as f:
                for line in f.readlines():
                    uid_vad = line.split()
                    uid, vad_offset = uid_vad
                    uid2vad[uid] = vad_offset

        total_frames = 0
        self.utt2num_frames = {}
        if self.sample_type != 'balance':
            if os.path.exists(utt2num_frames):
                with open(utt2num_frames, 'r') as f:
                    for l in f.readlines():
                        uid, num_frames = l.split()
                        if uid in uid2vad:
                            num_frames = np.sum(kaldiio.load_mat(uid2vad[uid]))
                        if feat_type == 'wav':
                            num_frames = float(num_frames) * sr

                        num_frames = int(num_frames)
                        self.utt2num_frames[uid] = num_frames
                        if num_frames < self.min_frames:
                            invalid_uid.add(uid)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split()
                spk_name = spk_utt[0]
                if spk_name not in dataset:
                    dataset[spk_name] = [
                        x for x in spk_utt[1:] if x not in invalid_uid]

        utt2spk_dict = {}
        with open(utt2spk, 'r') as u:
            for line in u.readlines():
                uid, sid = line.split()
                # if uid in invalid_uid:
                #     continue
                if uid not in utt2spk_dict:
                    utt2spk_dict[uid] = sid

        self.dom_to_idx = None
        self.utt2dom_dict = None
        dom_to_idx = None
        if self.domain:
            assert os.path.exists(utt2dom), utt2dom
            utt2dom_dict = {}
            with open(utt2dom, 'r') as u:
                for line in u.readlines():
                    utt_dom = line.split()
                    uid = utt_dom[0]
                    if uid in invalid_uid:
                        continue
                    if uid not in utt2dom_dict:
                        utt2dom_dict[uid] = utt_dom[-1]

            all_domains = [utt2dom_dict[u] for u in utt2dom_dict.keys()]
            domains = list(set(all_domains))
            domains.sort()
            dom_to_idx = {domains[i]: i for i in range(len(domains))}
            self.dom_to_idx = dom_to_idx
            if verbose > 1:
                print("Domain idx: ", str(self.dom_to_idx))
            self.utt2dom_dict = utt2dom_dict

        # pdb.set_trace()
        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        self.num_spks = len(speakers)
        if verbose > 0:
            print('==> There are {} speakers in Dataset.'.format(len(speakers)))

        if not os.path.exists(os.path.join(save_dir, 'spk2idx')):
            spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
            with open(os.path.join(save_dir, 'spk2idx'), 'w') as f:
                json.dump(spk_to_idx, f)
        else:
            with open(os.path.join(save_dir, 'spk2idx'), 'r') as f:
                spk_to_idx = json.load(f)

        idx_to_spk = {spk_to_idx[sid]: sid for sid in spk_to_idx}
        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        with open(feat_scp, 'r') as f:
            for line in f.readlines():
                uid_feat = line.split()
                if len(uid_feat) == 2:
                    uid, feat_offset = uid_feat
                elif len(uid_feat) == 3:
                    uid, _, feat_offset = uid_feat
                elif len(uid_feat) > 3:
                    uid = uid_feat[0]
                    feat_offset = uid_feat[4]

                if uid in invalid_uid:
                    continue
                uid2feat[uid] = feat_offset

        if verbose > 0:
            print('    There are {} utterances in Trainset, where {} utterances are removed.'.format(len(uid2feat),
                                                                                                          len(invalid_uid)))
        self.valid_set = None
        self.valid_uid2feat = None
        self.valid_utt2spk_dict = None
        self.valid_utt2dom_dict = None
        self.valid_base_utts = None

        if num_valid > 0:
            valid_set = {}
            valid_uid2feat = {}
            valid_utt2spk_dict = {}
            valid_utt2dom_dict = {}
            valid_base_utts = []

            if not os.path.exists(os.path.join(save_dir, 'valid.csv')):
                for spk in speakers:
                    if spk not in valid_set.keys():
                        valid_set[spk] = []
                        if isinstance(num_valid, float) and num_valid < 1.0:
                            numofutt = len(dataset[spk]) - int(np.ceil((1-num_valid) * len(dataset[spk])))
                        else:
                            numofutt = num_valid

                        for i in range(numofutt):
                            j = np.random.randint(len(dataset[spk]))
                            utt = dataset[spk].pop(j)
                            valid_set[spk].append(utt)

                            valid_uid2feat[valid_set[spk][-1]
                                        ] = uid2feat.pop(valid_set[spk][-1])
                            valid_utt2spk_dict[utt] = utt2spk_dict[utt]
                            if self.domain:
                                valid_utt2dom_dict[utt] = utt2dom_dict[utt]

                for utt in valid_uid2feat:
                    num_frames = self.utt2num_frames[utt]
                    this_numofseg = int(np.ceil(float(num_frames-segment_len+segment_shift) / segment_shift))

                    for i in range(this_numofseg):
                        start = int(i * segment_shift)
                        end = int(min(start+segment_len, num_frames))
                        start = int(max(end - num_frames, 0))

                        if (end - start) >= (segment_len*0.125) :
                            valid_base_utts.append((utt, start, end))

                valid_utts = pd.DataFrame(valid_base_utts, columns=['uid', 'start', 'end'])
                if save_dir != '':
                    valid_utts.to_csv(os.path.join(save_dir, 'valid.csv'), index=None)
            else:
                valid_base_utts = pd.read_csv(os.path.join(save_dir, 'valid.csv')).to_numpy().tolist()
                valid_uids = set([utt for utt, start, end in valid_base_utts])
                for uid in valid_uids:
                    valid_set.setdefault(utt2spk_dict[uid], []).extend([uid])
                    valid_utt2spk_dict[uid] = utt2spk_dict[uid]
                    valid_uid2feat[uid] = uid2feat[uid]
                    if self.domain:
                        valid_utt2dom_dict[uid] = utt2dom_dict[uid]

            if verbose > 0:
                print('    Spliting {} utterances for Validation.'.format(
                    len(valid_uid2feat)))
                
            self.valid_set = valid_set
            self.valid_uid2feat = valid_uid2feat
            self.valid_utt2spk_dict = valid_utt2spk_dict
            self.valid_uids = set(list(valid_utt2spk_dict.keys()))
            self.valid_utt2dom_dict = valid_utt2dom_dict
            self.valid_base_utts = valid_base_utts

        if os.path.exists(os.path.join(save_dir, 'train.csv')):
            if verbose > 0:
                print('    Loading training samples from:\n\t {} '.format(os.path.join(save_dir, 'train.csv')))
            train_base_utts = pd.read_csv(os.path.join(save_dir, 'train.csv')).to_numpy().tolist()
        else:
            train_base_utts = []
            for sid in speakers:
                for uid in dataset[sid]:
                    num_frames = self.utt2num_frames[uid]
                    this_numofseg = int(np.ceil(float(num_frames-segment_len+segment_shift) / segment_shift))

                    for i in range(this_numofseg):
                        start = int(i * segment_shift)
                        end = int(min(start+segment_len, num_frames))
                        start = int(max(end - num_frames, 0))

                        if (end - start) >= (segment_len*0.125) :
                            train_base_utts.append((uid, start, end))

            # self.base_utts = train_base_utts
            if verbose > 0:
                print('    There are {} basic segments for training .'.format(len(train_base_utts)))
            # random.shuffle(train_base_utts)
            if self.sample_type != 'instance':
                while len(train_base_utts) < samples_per_speaker * self.num_spks:
                    sid_idx = len(train_base_utts) % self.num_spks
                    sid = idx_to_spk[sid_idx]
                    uid = np.random.choice(dataset[sid])

                    this_frames = self.utt2num_frames[uid]

                    if this_frames <= self.segment_len:
                        start = 0 
                        end = this_frames
                    else:
                        start = np.random.randint(0, this_frames-self.segment_len)
                        end = start + self.segment_len

                    if (end - start) >= self.min_frames:
                        train_base_utts.append((uid, start, end))

                assert len(train_base_utts) == samples_per_speaker * self.num_spks
            random.shuffle(train_base_utts)
            if save_dir != '':
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                if torch.distributed.is_initialized() and torch.distributed.get_rank() != 0:
                    pass
                else:
                    train_utts = pd.DataFrame(train_base_utts, columns=['uid', 'start', 'end'])
                    train_utts.to_csv(os.path.join(save_dir, 'train.csv'), index=None)
                    if verbose > 0:
                        print('    Saving train.csv to {}.'.format(os.path.join(save_dir, 'train.csv')))

        self.base_utts = train_base_utts
        self.speakers = speakers
        self.utt2spk_dict = utt2spk_dict
        self.dataset = dataset
        self.uid2feat = uid2feat
        self.uid2vad = uid2vad
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_doms = len(self.dom_to_idx) if dom_to_idx != None else 0
        self.loader = loader
        self.feat_dim = loader(uid2feat[list(uid2feat.keys())[0]]).shape[-1]
        self.transform = transform

        if sample_type == 'instance':
            if verbose > 1:
                print(
                    '    The number of samples is related to the number of total frames.')

        elif samples_per_speaker == 0:
            samples_per_speaker = np.power(2, np.ceil(
                np.log2(total_frames * 2 / c.NUM_FRAMES_SPECT / self.num_spks)))
            if verbose > 1:
                print(
                    '    The number of samples for each speakers is decided by the number of total frames.')
        else:
            samples_per_speaker = max(
                np.ceil(len(base_utts) / len(speakers)), samples_per_speaker)
            if verbose > 1:
                print(
                    '    The number of samples for each speakers is add to the number of total frames.')

        self.samples_per_speaker = int(samples_per_speaker)
        self.c_axis = 0 if feat_type != 'wav' else 1
        self.feat_shape = (0, self.feat_dim) if feat_type != 'wav' else (1, 0)
        if verbose > 0:
            print('    Sample {} * {} = {} segments for speakers.'.format(
                self.samples_per_speaker, len(self.speakers), self.samples_per_speaker * len(self.speakers)))

        if self.return_uid or self.domain:
            self.utt_dataset = []
            for i in range(self.num_spks):
                # self.samples_per_speaker
                sid = i % self.num_spks
                spk = self.idx_to_spk[sid]
                utts = self.dataset[spk]
                
                uids = utts[:self.samples_per_speaker]
                for uid in uids:
                    self.utt_dataset.append([uid, sid])

    def __getitem__(self, idx):
        # start_time = time.time()
        if self.return_uid or self.domain:
            uid, label = self.utt_dataset[idx]
            y = self.loader(self.uid2feat[uid])
            feature = self.transform(y)

            if self.domain:
                label_b = self.dom_to_idx[self.utt2dom_dict[uid]]
                return feature, label, label_b
            else:
                return feature, label, uid

        (uid, start, end) = self.base_utts[idx]
        if self.feat_type != 'wav':
            y = self.loader(self.uid2feat[uid])
        else:
            y = self.loader(
                self.uid2feat[uid], start=start, stop=end)

            if uid in self.uid2vad:
                voice_idx = np.where(
                    kaldiio.load_mat(self.uid2vad[uid]) == 1)[0]
                y = y[voice_idx]

        sid = self.utt2spk_dict[uid]
        label = self.spk_to_idx[sid]
        feature = self.transform(y)

        return feature, label

    def __len__(self):
        if self.return_uid or self.domain:
            return len(self.utt_dataset)
        
        elif self.sample_type == 'instance':
            return len(self.base_utts)
        # elif self.sample_type == 'half_balance':
        #     return max(len(self.base_utts), self.samples_per_speaker * len(self.speakers))
        else:
            # 返回一个epoch的采样数
            return self.samples_per_speaker * len(self.speakers)


class ScriptValidDataset(data.Dataset):
    def __init__(self, valid_set, spk_to_idx, valid_uid2feat, valid_utt2spk_dict,
                 transform, dom_to_idx=None, valid_utt2dom_dict=None, loader=np.load,
                 save_dir='', valid_base_utts=None, feat_type='kaldi',
                 return_uid=False, domain=False, verbose=1):
        speakers = [spk for spk in valid_set.keys()]
        speakers.sort()
        self.feat_type = feat_type
        self.dom_to_idx = dom_to_idx
        self.utt2dom_dict = valid_utt2dom_dict

        if valid_base_utts == None:
            if os.path.exists(os.path.join(save_dir, 'valid.csv')):
                valid_base_utts = pd.read_csv(os.path.join(save_dir, 'valid.csv')).to_numpy().tolist()
        
        self.valid_base_utts = valid_base_utts
        self.speakers = speakers
        self.valid_set = valid_set
        self.uid2feat = valid_uid2feat
        self.domain = domain

        uids = list(valid_uid2feat.keys())
        uids.sort()
        if verbose > 0:
            print('Examples uids: ', uids[:4])

        self.uids = uids
        self.utt2spk_dict = valid_utt2spk_dict
        self.spk_to_idx = spk_to_idx
        self.num_spks = len(speakers)

        self.loader = loader
        self.transform = transform
        self.return_uid = return_uid

    def __getitem__(self, index):
        if self.valid_base_utts != None:
            (uid, start, end) = self.valid_base_utts[index]
            if self.feat_type != 'wav':
                y = self.loader(self.uid2feat[uid])
            else:
                y = self.loader(
                    self.uid2feat[uid], start=start, stop=end)
        else:
            uid = self.uids[index]
            y = self.loader(self.uid2feat[uid])

        spk = self.utt2spk_dict[uid]
        feature = self.transform(y)
        label = self.spk_to_idx[spk]

        if self.domain:
            return feature, label, self.dom_to_idx[self.utt2dom_dict[uid]]

        if self.return_uid:
            return feature, label, uid

        return feature, label

    def __len__(self):
        if self.valid_base_utts != None:
            return len(self.valid_base_utts)
        else:
            return len(self.uids)


class ScriptEvalDataset(data.Dataset):
    # dataset for inset and delete
    def __init__(self, select_dir, valid_dir, transform, feat_type='kaldi',
                 return_uid=False, verbose=1):
        
        uid_file = os.path.join(select_dir, 'uid_idx.json')
        with open(uid_file, 'r') as f:
            uids = json.load(f)

        self.data_file = os.path.join(select_dir, 'data.h5py')
        self.grad_file = os.path.join(valid_dir, 'grad.h5py')

        # uids.sort()
        if verbose > 0:
            print('Examples uids: ', uids[:2])

        self.uids = uids
        self.transform = transform

    def __getitem__(self, index):
        uid, idx = self.uids[index]

        with h5py.File(self.data_file, 'r') as r:
            data = r.get(uid)[:]

        with h5py.File(self.grad_file, 'r') as r:
            grad = r.get(uid)[:]

        if data.shape[0] > grad.shape[0]:
            data = data[:grad.shape[0]]

        data = self.transform((data, grad))
        return data, idx
        

    def __len__(self):
        return len(self.uids)


class ScriptTestDataset(data.Dataset):
    def __init__(self, dir, transform, loader=np.load, return_uid=False):

        feat_scp = dir + '/feats.scp'
        spk2utt = dir + '/spk2utt'
        trials = dir + '/trials'

        if not os.path.exists(feat_scp):
            raise FileExistsError(feat_scp)
        if not os.path.exists(trials):
            raise FileExistsError(trials)

        if os.path.exists(spk2utt):
            speakers = []
            with open(spk2utt, 'r') as u:
                all_cls = u.readlines()
                for line in all_cls:
                    spk_utt = line.split(' ')
                    spk_name = spk_utt[0]
                    speakers.append(spk_name)
            speakers.sort()
            self.speakers = speakers
            self.num_spks = len(speakers)
            print('    There are {} speakers in Test Dataset.'.format(len(speakers)))

        uid2feat = {}
        with open(feat_scp, 'r') as f:
            for line in f.readlines():
                uid, feat_offset = line.split()
                uid2feat[uid] = feat_offset

        print('    There are {} utterances in Test Dataset.'.format(len(uid2feat)))

        trials_pair = []
        positive_pairs = 0
        with open(trials, 'r') as t:
            all_pairs = t.readlines()
            for line in all_pairs:
                pair = line.split()
                if pair[2] == 'nontarget' or pair[2] == '0':
                    pair_true = False
                else:
                    pair_true = True
                    positive_pairs += 1

                trials_pair.append((pair[0], pair[1], pair_true))

        trials_pair = np.array(trials_pair)
        trials_pair = trials_pair[trials_pair[:, 2].argsort()[::-1]]

        print('==>There are {} pairs in test Dataset with {} positive pairs'.format(
            len(trials_pair), positive_pairs))

        self.feat_dim = loader(uid2feat[list(uid2feat.keys())[0]]).shape[1]
        self.speakers = speakers
        self.uid2feat = uid2feat
        self.trials_pair = trials_pair
        self.num_spks = len(speakers)
        self.numofpositive = positive_pairs

        self.loader = loader
        self.transform = transform
        self.return_uid = return_uid

    def __getitem__(self, index):
        uid_a, uid_b, label = self.trials_pair[index]
        try:
            feat_a = self.uid2feat[uid_a]
            feat_b = self.uid2feat[uid_b]
            y_a = self.loader(feat_a)
            y_b = self.loader(feat_b)
        except Exception as e:
            print(feat_a, feat_b)
            raise e

        data_a = self.transform(y_a)
        data_b = self.transform(y_b)

        if label == 'True' or label == True:
            label = True
        else:
            label = False

        if self.return_uid:
            # pdb.set_trace()
            # print(uid_a, uid_b)
            return data_a, data_b, label, uid_a, uid_b

        return data_a, data_b, label

    def partition(self, num):
        random.seed(123456)

        if num > len(self.trials_pair):
            print('%d is greater than the total number of pairs')

        elif num * 0.3 > self.numofpositive:
            indices = list(range(self.numofpositive, len(self.trials_pair)))
            random.shuffle(indices)
            indices = indices[:(num - self.numofpositive)]
            positive_idx = list(range(self.numofpositive))

            positive_pairs = self.trials_pair[positive_idx].copy()
            nagative_pairs = self.trials_pair[indices].copy()

            self.trials_pair = np.concatenate(
                (positive_pairs, nagative_pairs), axis=0)
        else:
            indices = list(range(self.numofpositive, len(self.trials_pair)))
            random.shuffle(indices)
            indices = indices[:(num - int(0.3 * num))]

            positive_idx = list(range(self.numofpositive))
            random.shuffle(positive_idx)
            positive_idx = positive_idx[:int(0.3 * num)]
            positive_pairs = self.trials_pair[positive_idx].copy()
            nagative_pairs = self.trials_pair[indices].copy()

            self.numofpositive = len(positive_pairs)
            self.trials_pair = np.concatenate(
                (positive_pairs, nagative_pairs), axis=0)

        assert len(self.trials_pair) == num
        num_positive = 0
        for x, y, z in self.trials_pair:
            if z == 'True':
                num_positive += 1

        assert len(self.trials_pair) == num, '%d != %d' % (
            len(self.trials_pair), num)
        assert self.numofpositive == num_positive, '%d != %d' % (
            self.numofpositive, num_positive)
        print('    %d positive pairs remain.' % num_positive)

    def __len__(self):
        return len(self.trials_pair)


class AugTrainDataset(data.Dataset):
    def __init__(self, dir, sets, samples_per_speaker, transform, num_valid=5, feat_type='kaldi',
                 loader=np.load, return_uid=False, domain=False, rand_test=False, verbose=1):
        self.return_uid = return_uid
        self.domain = domain
        self.rand_test = rand_test
        self.sets = sets

        feat_scp = dir + '/feats.scp' if feat_type != 'wav' else dir + '/wav.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'
        utt2num_frames = dir + '/utt2num_frames'
        utt2dom = dir + '/utt2dom'

        assert os.path.exists(feat_scp), feat_scp
        assert os.path.exists(spk2utt), spk2utt

        invalid_uid = []
        total_frames = 0
        # if os.path.exists(utt2num_frames):
        #     with open(utt2num_frames, 'r') as f:
        #         for l in f.readlines():
        #             uid, num_frames = l.split()
        #             total_frames += int(num_frames)
        #             if int(num_frames) < 50:
        #                 invalid_uid.append(uid)

        dataset = {}
        with open(spk2utt, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                spk_utt = line.split()
                spk_name = spk_utt[0]
                if spk_name not in dataset:
                    # dataset[spk_name] = [x for x in spk_utt[1:] if x not in invalid_uid]
                    uniqe_utt = []
                    for x in spk_utt[1:]:
                        if x.split('-')[-1] not in self.sets:
                            uniqe_utt.append(x)

                    dataset[spk_name] = uniqe_utt

        utt2spk_dict = {}
        with open(utt2spk, 'r') as u:
            all_cls = u.readlines()
            for line in all_cls:
                utt_spk = line.split()
                uid = utt_spk[0]
                if uid in invalid_uid:
                    continue
                if uid not in utt2spk_dict:
                    utt2spk_dict[uid] = utt_spk[-1]

        self.dom_to_idx = None
        self.utt2dom_dict = None
        dom_to_idx = None
        if self.domain:
            assert os.path.exists(utt2dom), utt2dom

            utt2dom_dict = {}
            with open(utt2dom, 'r') as u:
                all_cls = u.readlines()
                for line in all_cls:
                    utt_dom = line.split()
                    uid = utt_dom[0]
                    if uid in invalid_uid:
                        continue
                    if uid not in utt2dom_dict:
                        utt2dom_dict[uid] = utt_dom[-1]

            all_domains = [utt2dom_dict[u] for u in utt2dom_dict.keys()]
            domains = list(set(all_domains))
            domains.sort()
            dom_to_idx = {domains[i]: i for i in range(len(domains))}
            self.dom_to_idx = dom_to_idx
            if verbose > 1:
                print("Domain idx: ", str(self.dom_to_idx))
            self.utt2dom_dict = utt2dom_dict

        # pdb.set_trace()

        speakers = [spk for spk in dataset.keys()]
        speakers.sort()
        if verbose > 0:
            print('==> There are {} speakers in Dataset.'.format(len(speakers)))
        spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
        idx_to_spk = {i: speakers[i] for i in range(len(speakers))}

        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        with open(feat_scp, 'r') as f:
            for line in f.readlines():
                uid, feat_offset = line.split()
                if uid in invalid_uid:
                    continue
                uid2feat[uid] = feat_offset

        if verbose > 0:
            print('    There are {} utterances in Train Dataset, where {} utterances are removed.'.format(len(uid2feat),
                                                                                                          len(invalid_uid)))
        self.valid_set = None
        self.valid_uid2feat = uid2feat
        self.valid_utt2spk_dict = None
        self.valid_utt2dom_dict = None

        if num_valid > 0:
            valid_set = {}
            valid_uid2feat = {}
            valid_utt2spk_dict = {}
            valid_utt2dom_dict = {}

            for spk in speakers:
                if spk not in valid_set.keys():
                    valid_set[spk] = []
                    for i in range(num_valid):
                        if len(dataset[spk]) <= 1:
                            break
                        j = np.random.randint(len(dataset[spk]))
                        utt = dataset[spk].pop(j)
                        valid_set[spk].append(utt)

                        valid_utt2spk_dict[utt] = utt2spk_dict[utt]
                        if self.domain:
                            valid_utt2dom_dict[utt] = utt2dom_dict[utt]

            if verbose > 0:
                print('    Spliting {} utterances for Validation.'.format(
                    len(valid_uid2feat)))
            self.valid_set = valid_set
            self.valid_uid2feat = uid2feat
            self.valid_utt2spk_dict = valid_utt2spk_dict
            self.valid_utt2dom_dict = valid_utt2dom_dict

        self.speakers = speakers
        self.dataset = dataset
        self.uid2feat = uid2feat
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_spks = len(speakers)
        self.num_doms = len(self.dom_to_idx) if dom_to_idx != None else 0

        self.loader = loader
        self.feat_dim = loader(uid2feat[list(uid2feat.keys())[0]]).shape[-1]
        self.transform = transform
        if samples_per_speaker == 0:
            samples_per_speaker = np.power(2, np.ceil(
                np.log2(total_frames * 2 / c.NUM_FRAMES_SPECT / self.num_spks)))
            if verbose > 1:
                print(
                    '    The number of sampling utterances for each speakers is decided by the number of total frames.')
        self.samples_per_speaker = int(samples_per_speaker)
        self.c_axis = 0 if feat_type != 'wav' else 1
        self.feat_shape = (0, self.feat_dim) if feat_type != 'wav' else (1, 0)
        if verbose > 0:
            print('    Sample {} random utterances for each speakers.'.format(
                self.samples_per_speaker))

        if self.return_uid or self.domain:
            self.utt_dataset = []
            for i in range(self.samples_per_speaker * self.num_spks):
                sid = i % self.num_spks
                spk = self.idx_to_spk[sid]
                utts = self.dataset[spk]
                uid = utts[random.randrange(0, len(utts))]
                self.utt_dataset.append([uid, sid])

    def __getitem__(self, sid):
        # start_time = time.time()
        if self.return_uid or self.domain:
            uid, label = self.utt_dataset[sid]
            y = self.loader(self.uid2feat[uid])
            feature = self.transform(y)

            if self.domain:
                label_b = self.dom_to_idx[self.utt2dom_dict[uid]]
                return feature, label, label_b
            else:
                return feature, label, uid

        rand_idxs = [sid]
        sid %= self.num_spks
        spk = self.idx_to_spk[sid]
        utts = self.dataset[spk]
        num_utt = len(utts)

        y1 = np.array([[]]).reshape(0, self.feat_dim)
        y2 = np.array([[]]).reshape(0, self.feat_dim)

        rand_utt_idx = np.random.randint(0, num_utt)
        rand_idxs.append(rand_utt_idx)
        uid = utts[rand_utt_idx]
        this_set = random.choice(self.sets)
        uid_aug = utts[rand_utt_idx] + '-' + \
            this_set if this_set != 'none' else utts[rand_utt_idx]

        feature = self.loader(self.uid2feat[uid])
        y1 = np.concatenate((y1, feature), axis=0)

        feature = self.loader(self.uid2feat[uid_aug])
        y2 = np.concatenate((y2, feature), axis=0)

        while len(y1) < c.N_SAMPLES:
            rand_utt_idx = np.random.randint(0, num_utt)
            rand_idxs.append(rand_utt_idx)

            uid = utts[rand_utt_idx]
            this_set = random.choice(self.sets)
            uid_aug = utts[rand_utt_idx] + '-' + \
                this_set if this_set != 'none' else utts[rand_utt_idx]

            feature = self.loader(self.uid2feat[uid])
            y1 = np.concatenate((y1, feature), axis=0)

            feature = self.loader(self.uid2feat[uid_aug])
            y2 = np.concatenate((y2, feature), axis=0)

            # transform features if required
        # if self.rand_test:
        #     while len(rand_idxs) < 4:
        #         rand_idxs.append(-1)
        #
        #     start, length = self.transform(y)
        #     rand_idxs.append(start)
        #     rand_idxs.append(length)
        #
        #     # [uttid uttid -1 -1 start lenght]
        #     return torch.tensor(rand_idxs).reshape(1, -1), sid

        feature1 = self.transform(y1)
        feature2 = self.transform(y2)

        # print(feature1.shape)
        feature = np.concatenate([feature1, feature2], axis=0)
        # label = sid

        return feature, sid

    def __len__(self):
        return self.samples_per_speaker * len(self.speakers)  # 返回一个epoch的采样数


class AugValidDataset(data.Dataset):
    def __init__(self, valid_set, sets, spk_to_idx, valid_uid2feat, valid_utt2spk_dict,
                 transform, dom_to_idx=None, valid_utt2dom_dict=None, loader=np.load,
                 return_uid=False, domain=False, verbose=1):
        speakers = [spk for spk in valid_set.keys()]
        speakers.sort()

        self.dom_to_idx = dom_to_idx
        self.utt2dom_dict = valid_utt2dom_dict

        self.speakers = speakers
        self.dataset = valid_set
        self.valid_set = valid_set
        self.uid2feat = valid_uid2feat
        self.domain = domain
        self.sets = sets

        uids = []
        for spk in self.dataset:
            for u in self.dataset[spk]:
                uids.append(u)

        uids.sort()
        if verbose > 0:
            print('Examples uids: ', uids[:5])

        self.uids = uids
        self.utt2spk_dict = valid_utt2spk_dict
        self.spk_to_idx = spk_to_idx
        self.num_spks = len(speakers)

        self.loader = loader
        self.transform = transform
        self.return_uid = return_uid

    def __getitem__(self, index):
        uid = self.uids[index]
        this_set = random.choice(self.sets)
        uid_aug = uid + '-' + this_set if this_set != 'none' else uid

        spk = self.utt2spk_dict[uid]
        y1 = self.loader(self.uid2feat[uid])
        y2 = self.loader(self.uid2feat[uid_aug])

        feature1 = self.transform(y1)
        feature2 = self.transform(y2)

        # feature = torch.cat([feature1, feature2], dim=1)
        feature = np.concatenate([feature1, feature2], axis=0)
        label = self.spk_to_idx[spk]

        if self.domain:
            return feature, label, self.dom_to_idx[self.utt2dom_dict[uid]]

        if self.return_uid:
            return feature, label, uid

        return feature, label

    def __len__(self):
        return len(self.uids)


class SitwTestDataset(data.Dataset):
    """

    """

    def __init__(self, sitw_dir, sitw_set, transform, loader=kaldiio.load_mat, return_uid=False, set_suffix='no_sil'):
        # sitw_set: dev, eval
        feat_scp = sitw_dir + '/%s%s/feats.scp' % (sitw_set, set_suffix)
        spk2utt = sitw_dir + '/%s%s/spk2utt' % (sitw_set, set_suffix)
        trials = sitw_dir + '/%s%s/trials' % (sitw_set, set_suffix)

        for p in feat_scp, spk2utt, trials:
            check_exist(p)

        uid2feat = {}
        with open(feat_scp, 'r') as t:
            all_pairs = t.readlines()
            for line in all_pairs:
                # 12013 lpnns target
                pair = line.split()
                uid2feat[pair[0]] = pair[1]

        trials_pair = []
        numofpositive = 0
        with open(trials, 'r') as t:
            all_pairs = t.readlines()
            for line in all_pairs:
                # 12013 lpnns target
                pair = line.split()
                if pair[2] == 'nontarget':
                    pair_true = False
                else:
                    pair_true = True
                    numofpositive += 1

                trials_pair.append((pair[0], pair[1], pair_true))

        trials_pair = np.array(trials_pair)
        trials_pair = trials_pair[trials_pair[:, 2].argsort()[::-1]]

        print('==>There are %d pairs in sitw %s Dataset %d of them are positive.' % (
            len(trials_pair), sitw_set, numofpositive))
        # pdb.set_trace()
        self.feat_dim = loader(uid2feat[trials_pair[0][0]]).shape[1]

        self.pairs = len(trials_pair)
        self.numofpositive = numofpositive
        self.uid2feat = uid2feat
        self.trials_pair = trials_pair
        self.loader = loader
        self.transform = transform
        self.return_uid = return_uid

    def __getitem__(self, index):
        uid_a, uid_b, label = self.trials_pair[index]

        data_a = self.loader(self.uid2feat[uid_a])
        data_b = self.loader(self.uid2feat[uid_b])

        data_a = self.transform(data_a)
        data_b = self.transform(data_b)
        if label == 'True' or label == True:
            label = True
        else:
            label = False

        if self.return_uid:
            return data_a, data_b, label, uid_a, uid_b

        return data_a, data_b, label

    def partition(self, num):
        if num > self.pairs:
            print('%d is greater than the total number of pairs')

        elif num * 0.3 > self.numofpositive:
            indices = list(range(self.numofpositive, len(self.trials_pair)))
            random.shuffle(indices)
            indices = indices[:(num - self.numofpositive)]
            positive_idx = list(range(self.numofpositive))

            positive_pairs = self.trials_pair[positive_idx].copy()
            nagative_pairs = self.trials_pair[indices].copy()

            self.trials_pair = np.concatenate(
                (positive_pairs, nagative_pairs), axis=0)
        else:
            indices = list(range(self.numofpositive, len(self.trials_pair)))
            random.shuffle(indices)
            indices = indices[:(num - int(0.3 * num))]

            positive_idx = list(range(self.numofpositive))
            random.shuffle(positive_idx)
            positive_idx = positive_idx[:int(0.3 * num)]
            positive_pairs = self.trials_pair[positive_idx].copy()
            nagative_pairs = self.trials_pair[indices].copy()

            self.numofpositive = len(positive_pairs)
            self.trials_pair = np.concatenate(
                (positive_pairs, nagative_pairs), axis=0)

        assert len(self.trials_pair) == num
        num_positive = 0
        for x, y, z in self.trials_pair:
            if z == 'True':
                num_positive += 1

        assert len(self.trials_pair) == num, '%d != %d' % (
            len(self.trials_pair), num)
        assert self.numofpositive == num_positive, '%d != %d' % (
            self.numofpositive, num_positive)
        print('%d positive pairs remain.' % num_positive)

    def __len__(self):
        return len(self.trials_pair)


class my_data_prefetcher:
    def __init__(self, loader, gpu):
        self.loader = iter(loader)
        self.gpu = gpu
        # if self.gpu:
        #     self.stream = torch.cuda.Stream()
        self.preload()

    def __iter__(self):
        return self

    def preload(self):
        try:
            self.next_data = next(self.loader)
        except StopIteration:
            self.next_input = None
            raise StopIteration

        # if self.gpu:
        #     with torch.cuda.stream(self.stream):
        #         for i in range(len(self.next_data)):
        #             self.next_data[i] = self.next_data[i].cuda(non_blocking=True)

    def __next__(self):
        # if self.gpu:
        #     torch.cuda.current_stream().wait_stream(self.stream)

        data = self.next_data
        self.preload()

        return data


class PairTrainDataset(data.Dataset):
    def __init__(self, dir, miss_trials, transform, feat_type='kaldi',
                 loader=np.load, return_uid=False, domain=False, rand_test=False,
                 vad_select=False, target_ratio=0.5,
                 segment_len=c.N_SAMPLES, verbose=1):
        self.return_uid = return_uid
        self.domain = domain
        self.rand_test = rand_test
        self.segment_len = segment_len

        feat_scp = dir + '/feats.scp' if feat_type != 'wav' else dir + '/wav.scp'
        spk2utt = dir + '/spk2utt'
        utt2spk = dir + '/utt2spk'
        utt2num_frames = dir + '/utt2num_frames'
        utt2dom = dir + '/utt2dom'
        vad_scp = dir + '/vad.scp'

        assert os.path.exists(feat_scp), feat_scp
        assert os.path.exists(spk2utt), spk2utt

        invalid_uid = []
        base_utts = []

        uid2vad = {}
        if vad_select:
            assert os.path.exists(vad_scp), vad_scp
            # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
            with open(vad_scp, 'r') as f:
                for line in f.readlines():
                    uid, vad_offset = line.split()

                    if uid in invalid_uid:
                        continue

                    uid2vad[uid] = vad_offset

        total_frames = 0
        if os.path.exists(utt2num_frames):
            with open(utt2num_frames, 'r') as f:
                for l in f.readlines():
                    uid, num_frames = l.split()

                    if uid in uid2vad:
                        num_frames = np.sum(kaldiio.load_mat(uid2vad[uid]))
                    else:
                        num_frames = int(num_frames)

                    total_frames += num_frames
                    this_numofseg = int(
                        np.ceil(float(num_frames) / segment_len))

                    for i in range(this_numofseg):
                        end = min((i + 1) * segment_len, num_frames)
                        start = min(end - segment_len, 0)
                        base_utts.append((uid, start, end))
            if verbose > 0:
                print('    There are {} basic segments.'.format(len(base_utts)))

                # if int(num_frames) < 50:
                #     invalid_uid.append(uid)
        self.base_utts = base_utts
        # dataset = {}
        # with open(spk2utt, 'r') as u:
        #     all_cls = u.readlines()
        #     for line in all_cls:
        #         spk_utt = line.split()
        #         spk_name = spk_utt[0]
        #         if spk_name not in dataset:
        #             dataset[spk_name] = [x for x in spk_utt[1:] if x not in invalid_uid]

        utt2spk_dict = {}
        speakers = set([])

        with open(utt2spk, 'r') as u:
            for line in u.readlines():
                uid, sid = line.split()
                speakers.add(sid)
                if uid in invalid_uid:
                    continue
                if uid not in utt2spk_dict:
                    utt2spk_dict[uid] = sid

        speakers = list(speakers)
        speakers.sort()

        self.dom_to_idx = None
        self.utt2dom_dict = None
        dom_to_idx = None
        if self.domain:
            assert os.path.exists(utt2dom), utt2dom

            utt2dom_dict = {}
            with open(utt2dom, 'r') as u:
                for line in u.readlines():
                    utt_dom = line.split()
                    uid = utt_dom[0]
                    if uid in invalid_uid:
                        continue
                    if uid not in utt2dom_dict:
                        utt2dom_dict[uid] = utt_dom[-1]

            all_domains = [utt2dom_dict[u] for u in utt2dom_dict.keys()]
            domains = list(set(all_domains))
            domains.sort()
            dom_to_idx = {domains[i]: i for i in range(len(domains))}
            self.dom_to_idx = dom_to_idx
            if verbose > 1:
                print("Domain idx: ", str(self.dom_to_idx))
            self.utt2dom_dict = utt2dom_dict

        # pdb.set_trace()

        if verbose > 0:
            print('==> There are {} speakers in Dataset.'.format(len(speakers)))
        spk_to_idx = {speakers[i]: i for i in range(len(speakers))}
        idx_to_spk = {i: speakers[i] for i in range(len(speakers))}

        # 'Eric_McCormack-Y-qKARMSO7k-0001.wav': feature[frame_length, feat_dim]
        uid2feat = {}
        with open(feat_scp, 'r') as f:
            for line in f.readlines():
                uid_feat = line.split()
                if len(uid_feat) == 2:
                    uid, feat_offset = uid_feat
                else:
                    uid, _, feat_offset = uid_feat
                if uid in invalid_uid:
                    continue
                uid2feat[uid] = feat_offset

        dataset = []
        target = []
        nontarget = []
        with open(miss_trials, 'r') as f:
            for l in f.readlines():
                # pdb.set_trace()
                enroll_uid, eval_uid, truth = l.split()
                if enroll_uid in uid2feat and eval_uid in uid2feat:
                    # dataset.append((enroll_uid, eval_uid, truth))
                    if truth == 'target':
                        target.append((enroll_uid, eval_uid, truth))
                    else:
                        nontarget.append((enroll_uid, eval_uid, truth))

        i = 0
        random.shuffle(nontarget)
        for enroll_uid, eval_uid, truth in target:
            dataset.append((enroll_uid, eval_uid, truth))
            for j in range(int((1 - target_ratio) / target_ratio)):
                i += j
                i = i % len(nontarget)
                dataset.append(nontarget[i])

        if verbose > 0:
            print('    There are {} pairs in Train Dataset, {} of them are unique.'.format(len(dataset),
                                                                                           len(target) + len(
                                                                                               nontarget)))

        self.speakers = speakers
        self.utt2spk_dict = utt2spk_dict
        self.dataset = dataset
        self.uid2feat = uid2feat
        self.uid2vad = uid2vad
        self.spk_to_idx = spk_to_idx
        self.idx_to_spk = idx_to_spk
        self.num_spks = len(speakers)
        self.num_doms = len(self.dom_to_idx) if dom_to_idx != None else 0

        self.loader = loader
        self.feat_dim = loader(uid2feat[list(uid2feat.keys())[0]]).shape[-1]
        self.transform = transform

        self.c_axis = 0 if feat_type != 'wav' else 1
        self.feat_shape = (0, self.feat_dim) if feat_type != 'wav' else (1, 0)
        # if verbose > 0:
        #     print('    Sample {} random utterances for each speakers.'.format(self.samples_per_speaker))

    def __getitem__(self, sid):
        # start_time = time.time()

        enroll_uid, eval_uid, truth = self.dataset[sid]
        enroll_sid = self.utt2spk_dict[enroll_uid]
        eval_sid = self.utt2spk_dict[eval_uid]

        enroll_feature = self.transform(self.loader(self.uid2feat[enroll_uid]))
        eval_feature = self.transform(self.loader(self.uid2feat[eval_uid]))

        label = 1 if truth in ['True', 'true', 'target'] else 0

        return enroll_feature, eval_feature, label, self.spk_to_idx[enroll_sid], self.spk_to_idx[eval_sid]

    def __len__(self):
        return len(self.dataset)  # 返回一个epoch的采样数

# uid = ['A.J._Buckley-1zcIwhmdeo4-0001.wav', 'A.J._Buckley-1zcIwhmdeo4-0002.wav', 'A.J._Buckley-1zcIwhmdeo4-0003.wav', 'A.J._Buckley-7gWzIy6yIIk-0001.wav']
# xvector = np.random.randn(4, 512).astype(np.float32)
#
# ark_file = '../Data/xvector.ark'
# scp_file = '../Data/xvector.scp'
#
# write_xvector_ark(uid, xvector, ark_file, scp_file)
